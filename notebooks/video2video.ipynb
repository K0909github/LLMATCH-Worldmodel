{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":532},"id":"RlKHDjOCT3T4","executionInfo":{"status":"error","timestamp":1738644953037,"user_tz":-540,"elapsed":38603,"user":{"displayName":"K","userId":"12459954361559698098"}},"outputId":"e5609e2a-d53d-4bb6-cd3b-514a075b2ef7"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk0909\u001b[0m (\u001b[33munivercity-of-hyogo\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"stream","name":"stdout","text":["Model path /models/genrl_stickman_500k_2.pt\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-1-f6c005235aef>:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  agent = torch.load(agent_path)\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/models/genrl_stickman_500k_2.pt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f6c005235aef>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model path\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/models/genrl_stickman_500k_2.pt'"]}],"source":["from pathlib import Path\n","import os\n","import glob\n","import json\n","import sys\n","sys.path.append(str(Path(os.path.abspath('')).parent))\n","\n","import torch\n","import torch.distributions as D\n","import numpy as np\n","import torch.nn.functional as F\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import matplotlib.animation as animation\n","\n","import wandb\n","from tqdm import tqdm\n","api = wandb.Api()\n","\n","agent_path = Path(os.path.abspath('')).parent / 'models' / 'genrl_stickman_500k_2.pt'\n","print(\"Model path\", agent_path)\n","\n","agent = torch.load(agent_path)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":385},"id":"B8BIalw_T3UB","executionInfo":{"status":"error","timestamp":1738645024257,"user_tz":-540,"elapsed":530,"user":{"displayName":"K","userId":"12459954361559698098"}},"outputId":"0c786493-758d-41db-c86f-2ef07dcacd21"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'tools.genrl_utils'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-20050a97b92a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenrl_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mViCLIPGlobalInstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDOMAIN2PREDICATES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'viclip_model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'viclip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Get ViCLIP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'viclip_global_instance'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mviclip_global_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mviclip_global_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mViCLIPGlobalInstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tools.genrl_utils'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from tools.genrl_utils import ViCLIPGlobalInstance, DOMAIN2PREDICATES\n","model_name = getattr(agent.cfg, 'viclip_model', 'viclip')\n","# Get ViCLIP\n","if 'viclip_global_instance' not in locals() or model_name != viclip_global_instance._model:\n","    viclip_global_instance = ViCLIPGlobalInstance(model_name)\n","    if not viclip_global_instance._instantiated:\n","        print(\"Instantiating\")\n","        viclip_global_instance.instantiate()\n","    clip = viclip_global_instance.viclip\n","    tokenizer = viclip_global_instance.viclip_tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W94fDshWT3UE","executionInfo":{"status":"aborted","timestamp":1738644953039,"user_tz":-540,"elapsed":6,"user":{"displayName":"K","userId":"12459954361559698098"}}},"outputs":[],"source":["import cv2\n","\n","def get_vid_feat(frames, clip):\n","    return clip.get_vid_features(frames,)\n","\n","def _frame_from_video(video):\n","    while video.isOpened():\n","        success, frame = video.read()\n","        if success:\n","            yield frame\n","        else:\n","            break\n","\n","v_mean = np.array([0.485, 0.456, 0.406]).reshape(1,1,3)\n","v_std = np.array([0.229, 0.224, 0.225]).reshape(1,1,3)\n","def normalize(data):\n","    return (data/255.0-v_mean)/v_std\n","\n","def denormalize(data):\n","    return (((data * v_std) + v_mean) * 255)\n","\n","def frames2tensor(vid_list, fnum=8, target_size=(224, 224), device=torch.device('cuda')):\n","    vid_list = [*vid_list[0]]\n","    assert(len(vid_list) >= fnum)\n","    vid_list = [cv2.resize(x, target_size) for x in vid_list]\n","    vid_tube = [np.expand_dims(normalize(x), axis=(0, 1)) for x in vid_list]\n","    vid_tube = np.concatenate(vid_tube, axis=1)\n","    vid_tube = np.transpose(vid_tube, (0, 1, 4, 2, 3))\n","    vid_tube = torch.from_numpy(vid_tube).to(device, non_blocking=True).float()\n","    return vid_tube\n","\n","\n","def get_video_feat(frames, device=torch.device('cuda'), flip=False):\n","    # Image\n","    if frames.shape[1] == 1:\n","        frames = frames.transpose(1,0,2,3,4).repeat(8, axis=0).transpose(1,0,2,3,4)\n","\n","    # Short video\n","    if frames.shape[1] == 4:\n","        frames = frames.transpose(1,0,2,3,4).repeat(2, axis=0).transpose(1,0,2,3,4)\n","\n","    k = max(frames.shape[1] // 128, 1)\n","    frames = frames[:, ::k]\n","\n","    # Horizontally flip\n","    if flip:\n","        frames = np.flip(frames, axis=-2)\n","\n","    print(frames.shape,)\n","    chosen_frames = frames[:, :8]\n","    chosen_frames = frames2tensor(chosen_frames, device=device)\n","    vid_feat = get_vid_feat(chosen_frames, clip,)\n","    return vid_feat, chosen_frames\n","\n","VIDEO_PATH = Path(os.path.abspath('')).parent / 'assets' / 'video_samples'\n","video_name = 'headstand.mp4'\n","\n","video_file_path = str(VIDEO_PATH / video_name)\n","print(video_file_path)\n","video = cv2.VideoCapture(video_file_path)\n","frames = np.expand_dims(np.stack([ cv2.cvtColor(x, cv2.COLOR_BGR2RGB) for x in _frame_from_video(video)], axis=0), axis=0)\n","print('Video length:', frames.shape[1])\n","with torch.no_grad():\n","    vid_feat, frames_feat = get_video_feat(frames, flip=False)\n","print(vid_feat.shape)\n","plt.imshow(frames[0,0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rA57TaZOT3UH","executionInfo":{"status":"aborted","timestamp":1738644953039,"user_tz":-540,"elapsed":5,"user":{"displayName":"K","userId":"12459954361559698098"}}},"outputs":[],"source":["video_embed = vid_feat\n","DENOISE = True\n","\n","T = video_embed.shape[0]\n","\n","from torchvision.transforms import transforms as vision_trans\n","trasnf = vision_trans.Resize(size=(64, 64), interpolation=vision_trans.InterpolationMode.NEAREST)\n","\n","wm = world_model = agent.wm\n","connector = agent.wm.connector\n","decoder = world_model.heads['decoder']\n","n_frames = connector.n_frames\n","\n","\n","with torch.no_grad():\n","    # Get actions\n","    video_embed = video_embed.unsqueeze(1).repeat(1,n_frames, 1).reshape(1, n_frames * T, -1)\n","    action = wm.connector.get_action(video_embed)\n","\n","    # Imagine\n","    prior = wm.connector.video_imagine(video_embed, None, sample=False, reset_every_n_frames=False, denoise=DENOISE)\n","    prior_recon = decoder(wm.decoder_input_fn(prior))['observation'].mean + 0.5\n","\n","    # Plotting video\n","    ims = []\n","    fig, axes = plt.subplots(1, 1, figsize=(4, 8), frameon=False)\n","    fig.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, hspace = 0, wspace = 0)\n","    fig.set_size_inches(4,2)\n","\n","    for t in range(prior_recon.shape[1]):\n","        toadd = []\n","        for b in range(prior_recon.shape[0]):\n","            ax = axes\n","            ax.set_axis_off()\n","            img = cv2.resize((np.clip(prior_recon[b, t].cpu().permute(1,2,0), 0, 1).numpy() *255).astype(np.uint8), (224,224))\n","            orig_img = denormalize(frames_feat[b, t].cpu().permute(1,2,0) ).numpy().astype(np.uint8)\n","            frame =  ax.imshow(np.concatenate([orig_img, img], axis=1))\n","            toadd.append(frame) # add both the image and the text to the list of artists\n","        ims.append(toadd)\n","\n","    anim = animation.ArtistAnimation(fig, ims, interval=700, blit=True, repeat_delay=700, )\n","\n","    # Save GIFs\n","    writer = animation.PillowWriter(fps=15, metadata=dict(artist='Me'), bitrate=1800,)\n","    domain = agent.cfg.task.split('_')[0]\n","    os.makedirs(f'videos/{domain}/video2video', exist_ok=True)\n","    file_path = f'videos/{domain}/video2video/{video_name[:-4].replace(\" \",\"_\")}.gif'\n","    anim.save(file_path, writer=writer, )\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"}},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}